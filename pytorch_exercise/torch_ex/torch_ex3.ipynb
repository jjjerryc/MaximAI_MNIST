{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:  NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) \n",
      "Value: tensor([[-0.0204, -0.0210,  0.0082,  ...,  0.0049, -0.0189, -0.0153],\n",
      "        [ 0.0010, -0.0307,  0.0352,  ..., -0.0253, -0.0329,  0.0133]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) \n",
      "Value: tensor([ 0.0140, -0.0051], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) \n",
      "Value: tensor([[ 0.0290,  0.0138, -0.0265,  ..., -0.0292,  0.0087,  0.0182],\n",
      "        [-0.0339,  0.0119,  0.0174,  ..., -0.0302,  0.0121, -0.0088]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) \n",
      "Value: tensor([0.0192, 0.0338], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) \n",
      "Value: tensor([[ 0.0141, -0.0247, -0.0317,  ..., -0.0153,  0.0034, -0.0420],\n",
      "        [-0.0223, -0.0329,  0.0381,  ..., -0.0396, -0.0362, -0.0169]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) \n",
      "Value: tensor([ 0.0193, -0.0054], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU() \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "print(\"Model Structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} \\nValue: {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear weights: Parameter containing:\n",
      "tensor([[ 0.0137,  0.0280,  0.0353,  ...,  0.0257, -0.0059, -0.0207],\n",
      "        [ 0.0057,  0.0231,  0.0186,  ...,  0.0326, -0.0169, -0.0266],\n",
      "        [-0.0291,  0.0108,  0.0214,  ...,  0.0159, -0.0099, -0.0263],\n",
      "        ...,\n",
      "        [ 0.0090,  0.0169,  0.0129,  ...,  0.0160, -0.0120, -0.0326],\n",
      "        [ 0.0241,  0.0110,  0.0167,  ..., -0.0184, -0.0193, -0.0340],\n",
      "        [-0.0322, -0.0245,  0.0041,  ...,  0.0293, -0.0074,  0.0064]],\n",
      "       requires_grad=True) \n",
      "\n",
      "First Linear biases: Parameter containing:\n",
      "tensor([ 0.0146, -0.0102,  0.0287,  0.0203, -0.0060, -0.0042, -0.0320,  0.0002,\n",
      "        -0.0246, -0.0295,  0.0025, -0.0270, -0.0053, -0.0217, -0.0027, -0.0206,\n",
      "        -0.0265,  0.0260, -0.0104, -0.0155, -0.0267, -0.0192,  0.0303,  0.0110,\n",
      "         0.0152, -0.0078, -0.0008, -0.0252, -0.0343,  0.0166, -0.0351, -0.0121,\n",
      "         0.0043, -0.0168, -0.0041,  0.0318,  0.0163,  0.0027, -0.0098,  0.0348,\n",
      "         0.0215, -0.0217, -0.0253, -0.0014, -0.0009,  0.0159,  0.0343, -0.0160,\n",
      "        -0.0086,  0.0033, -0.0317, -0.0354,  0.0259,  0.0321,  0.0178, -0.0273,\n",
      "        -0.0296,  0.0217, -0.0317, -0.0116,  0.0307,  0.0016,  0.0314, -0.0011,\n",
      "        -0.0087, -0.0023,  0.0294, -0.0094,  0.0041, -0.0120, -0.0036, -0.0069,\n",
      "         0.0021, -0.0053, -0.0005,  0.0232, -0.0169, -0.0031, -0.0082, -0.0154,\n",
      "        -0.0130, -0.0179,  0.0314, -0.0236,  0.0049, -0.0341,  0.0334,  0.0348,\n",
      "         0.0294, -0.0125, -0.0265, -0.0332, -0.0261, -0.0087,  0.0320,  0.0277,\n",
      "        -0.0268,  0.0255,  0.0336,  0.0058, -0.0116,  0.0173, -0.0117,  0.0263,\n",
      "         0.0059, -0.0268,  0.0133,  0.0247, -0.0225,  0.0269,  0.0226, -0.0341,\n",
      "        -0.0233, -0.0321, -0.0326, -0.0220,  0.0281, -0.0210,  0.0196, -0.0045,\n",
      "        -0.0316,  0.0181,  0.0142,  0.0255, -0.0248, -0.0255,  0.0093,  0.0098,\n",
      "        -0.0167, -0.0298,  0.0090,  0.0047, -0.0061, -0.0068,  0.0289,  0.0341,\n",
      "        -0.0172, -0.0130, -0.0117,  0.0343,  0.0316,  0.0239,  0.0333,  0.0162,\n",
      "        -0.0011,  0.0299,  0.0003,  0.0045, -0.0173, -0.0022,  0.0168,  0.0097,\n",
      "         0.0217,  0.0022, -0.0009,  0.0118,  0.0053, -0.0190,  0.0033, -0.0155,\n",
      "        -0.0307,  0.0083,  0.0185, -0.0337,  0.0039,  0.0056, -0.0288,  0.0287,\n",
      "         0.0240, -0.0128,  0.0071,  0.0346,  0.0147,  0.0328, -0.0032,  0.0096,\n",
      "        -0.0181, -0.0254, -0.0247, -0.0277, -0.0094, -0.0184,  0.0153, -0.0325,\n",
      "         0.0265,  0.0232, -0.0288,  0.0175, -0.0355,  0.0329, -0.0039,  0.0160,\n",
      "        -0.0270, -0.0046, -0.0299,  0.0335,  0.0333, -0.0326,  0.0320, -0.0113,\n",
      "         0.0234, -0.0041,  0.0345,  0.0248, -0.0181, -0.0079,  0.0234,  0.0212,\n",
      "         0.0174,  0.0123,  0.0327,  0.0051,  0.0211,  0.0081, -0.0068, -0.0050,\n",
      "         0.0054,  0.0308, -0.0308, -0.0162,  0.0085, -0.0284, -0.0176,  0.0293,\n",
      "        -0.0242, -0.0039,  0.0332,  0.0336, -0.0073,  0.0044,  0.0187, -0.0291,\n",
      "         0.0233, -0.0197,  0.0037, -0.0347, -0.0335, -0.0041,  0.0123,  0.0046,\n",
      "        -0.0029,  0.0173, -0.0334,  0.0118, -0.0188, -0.0111,  0.0180, -0.0172,\n",
      "        -0.0163, -0.0203,  0.0102, -0.0354, -0.0157, -0.0166, -0.0275, -0.0117,\n",
      "         0.0338,  0.0066,  0.0337,  0.0087,  0.0011,  0.0045,  0.0292, -0.0213,\n",
      "         0.0095, -0.0183,  0.0285,  0.0352,  0.0176,  0.0249,  0.0354, -0.0337,\n",
      "        -0.0293, -0.0140, -0.0259,  0.0345, -0.0147,  0.0034,  0.0064,  0.0028,\n",
      "         0.0270,  0.0289, -0.0141,  0.0191,  0.0057, -0.0298, -0.0262, -0.0072,\n",
      "        -0.0187, -0.0278, -0.0026, -0.0325,  0.0097, -0.0157,  0.0043,  0.0154,\n",
      "         0.0181, -0.0308, -0.0147, -0.0219, -0.0052,  0.0210,  0.0083, -0.0331,\n",
      "         0.0203, -0.0160,  0.0226,  0.0031, -0.0111,  0.0244, -0.0158,  0.0016,\n",
      "         0.0331, -0.0097, -0.0233, -0.0356, -0.0222, -0.0266, -0.0330, -0.0111,\n",
      "        -0.0307, -0.0080, -0.0091,  0.0235, -0.0267,  0.0024,  0.0107, -0.0116,\n",
      "        -0.0243,  0.0118,  0.0198,  0.0127, -0.0191,  0.0231,  0.0124, -0.0270,\n",
      "        -0.0079,  0.0101,  0.0060,  0.0157, -0.0243, -0.0211,  0.0163, -0.0275,\n",
      "         0.0180, -0.0211, -0.0183, -0.0131,  0.0107, -0.0115, -0.0236,  0.0336,\n",
      "        -0.0294,  0.0201,  0.0355, -0.0206, -0.0206, -0.0015, -0.0150, -0.0148,\n",
      "        -0.0356, -0.0028,  0.0004,  0.0357,  0.0226,  0.0199, -0.0222, -0.0126,\n",
      "        -0.0185,  0.0073, -0.0302,  0.0122, -0.0221,  0.0118, -0.0242, -0.0204,\n",
      "        -0.0091,  0.0103, -0.0105,  0.0096, -0.0141,  0.0349,  0.0194,  0.0343,\n",
      "        -0.0241, -0.0109,  0.0226, -0.0274,  0.0336, -0.0213, -0.0258, -0.0088,\n",
      "         0.0060,  0.0054,  0.0060,  0.0232, -0.0314, -0.0299,  0.0055,  0.0069,\n",
      "        -0.0226, -0.0231, -0.0269,  0.0343,  0.0076,  0.0145,  0.0131, -0.0162,\n",
      "        -0.0065, -0.0039,  0.0184,  0.0266, -0.0247, -0.0305,  0.0290, -0.0064,\n",
      "        -0.0140, -0.0204, -0.0258, -0.0186,  0.0337, -0.0070, -0.0323, -0.0237,\n",
      "         0.0267,  0.0102, -0.0265,  0.0322,  0.0342, -0.0131, -0.0173, -0.0319,\n",
      "        -0.0220,  0.0002, -0.0024, -0.0315,  0.0322,  0.0049, -0.0091,  0.0260,\n",
      "        -0.0093,  0.0235, -0.0240,  0.0127, -0.0041,  0.0019,  0.0029, -0.0347,\n",
      "         0.0329,  0.0212,  0.0063,  0.0039, -0.0333, -0.0111, -0.0125, -0.0248,\n",
      "         0.0266, -0.0286, -0.0052, -0.0252, -0.0009,  0.0278, -0.0204, -0.0219,\n",
      "         0.0034, -0.0087, -0.0078, -0.0123,  0.0349,  0.0081, -0.0029,  0.0214,\n",
      "         0.0150, -0.0348,  0.0334,  0.0101,  0.0269, -0.0120,  0.0097,  0.0070,\n",
      "         0.0055,  0.0222, -0.0283,  0.0200, -0.0103,  0.0114,  0.0230, -0.0302,\n",
      "        -0.0144,  0.0168, -0.0042, -0.0110, -0.0147,  0.0197, -0.0123,  0.0155,\n",
      "         0.0008,  0.0039,  0.0054, -0.0123, -0.0147,  0.0131,  0.0273,  0.0218,\n",
      "         0.0219, -0.0109,  0.0075, -0.0119, -0.0120, -0.0234, -0.0248, -0.0243],\n",
      "       requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#see weight and bais\n",
    "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
    "\n",
    "print(f\"First Linear biases: {model.linear_relu_stack[0].bias} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n",
      "Before ReLU: tensor([[-0.5369,  0.1082,  0.0491, -0.0713, -0.2357,  0.1118,  0.4123, -0.3837,\n",
      "          0.0458,  0.3784,  0.3024,  0.4699,  0.0797,  0.4634, -0.0194,  0.0229,\n",
      "         -0.2723, -0.1575,  0.4920,  0.4679],\n",
      "        [-0.4735,  0.1043,  0.1683,  0.0927, -0.1149,  0.1623,  0.4244, -0.3641,\n",
      "         -0.1398,  0.3142,  0.2654,  0.2295,  0.1582,  0.4169, -0.0153,  0.1326,\n",
      "         -0.5122, -0.3191,  0.1845,  0.4811],\n",
      "        [-0.5697,  0.2815,  0.2723,  0.0789, -0.2452,  0.1002,  0.6495, -0.4420,\n",
      "          0.1343,  0.0836,  0.3504, -0.0972, -0.2470,  0.1731, -0.3145, -0.1599,\n",
      "          0.0207, -0.0460,  0.4405,  0.2721]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.1082, 0.0491, 0.0000, 0.0000, 0.1118, 0.4123, 0.0000, 0.0458,\n",
      "         0.3784, 0.3024, 0.4699, 0.0797, 0.4634, 0.0000, 0.0229, 0.0000, 0.0000,\n",
      "         0.4920, 0.4679],\n",
      "        [0.0000, 0.1043, 0.1683, 0.0927, 0.0000, 0.1623, 0.4244, 0.0000, 0.0000,\n",
      "         0.3142, 0.2654, 0.2295, 0.1582, 0.4169, 0.0000, 0.1326, 0.0000, 0.0000,\n",
      "         0.1845, 0.4811],\n",
      "        [0.0000, 0.2815, 0.2723, 0.0789, 0.0000, 0.1002, 0.6495, 0.0000, 0.1343,\n",
      "         0.0836, 0.3504, 0.0000, 0.0000, 0.1731, 0.0000, 0.0000, 0.0207, 0.0000,\n",
      "         0.4405, 0.2721]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#創造image\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "#convert each 2D 28x28 image into a contiguous array of 784 pixel values\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "#applies a linear transformation on the input using its stored weights and biases\n",
    "#the transformation is weight∗input+bias\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "\n",
    "#ReLU\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1128, 0.0000, 0.4026, 0.2332, 0.0692, 0.0000,\n",
      "         0.0692],\n",
      "        [0.0928, 0.0000, 0.0000, 0.0821, 0.0000, 0.4128, 0.1956, 0.0000, 0.0000,\n",
      "         0.0430],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0984, 0.0000, 0.3336, 0.2314, 0.0422, 0.0000,\n",
      "         0.0990]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0907, 0.0907, 0.0907, 0.1016, 0.0907, 0.1357, 0.1146, 0.0972, 0.0907,\n",
      "         0.0972],\n",
      "        [0.1002, 0.0913, 0.0913, 0.0991, 0.0913, 0.1379, 0.1110, 0.0913, 0.0913,\n",
      "         0.0953],\n",
      "        [0.0917, 0.0917, 0.0917, 0.1012, 0.0917, 0.1280, 0.1156, 0.0956, 0.0917,\n",
      "         0.1012]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#sequential model\n",
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features = 28*28, out_features = 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.ReLU()\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits.size())\n",
    "print(logits)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
